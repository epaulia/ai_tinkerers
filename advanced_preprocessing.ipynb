{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sberbank Housing - Advanced Preprocessing\n",
    "## Optimized for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor, OLSInfluence\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data & Log-Transform Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 27000 samples, 18 features\n",
      "  Target log-transformed\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data = pd.read_csv(r\"data/sberbank_housing.csv\", index_col=0, low_memory=False)\n",
    "data.columns = [c.lower().strip().replace(\" \", \"_\") for c in data.columns]\n",
    "data = data.drop(columns=[\"timestamp\", \"id\"])\n",
    "\n",
    "print(f\"Original: {data.shape[0]} samples, {data.shape[1]} features\")\n",
    "\n",
    "# Log-transform target\n",
    "data['price_doc'] = np.log1p(data['price_doc'])\n",
    "print(\"  Target log-transformed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Features Overview\n",
    "\n",
    "Examine categorical columns to determine encoding strategy based on cardinality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Categorical columns: 3\n",
      "============================================================\n",
      "\n",
      "product_type: [LOW cardinality]\n",
      "  Unique values: 2\n",
      "  Missing: 0 (0.0%)\n",
      "  Top 5 values:\n",
      "product_type\n",
      "    Investment       17542\n",
      "    OwnerOccupier     9458\n",
      "\n",
      "ecology: [LOW cardinality]\n",
      "  Unique values: 5\n",
      "  Missing: 0 (0.0%)\n",
      "  Top 5 values:\n",
      "ecology\n",
      "    poor            7215\n",
      "    no data         6570\n",
      "    good            6548\n",
      "    excellent       3419\n",
      "    satisfactory    3248\n",
      "\n",
      "sub_area: [HIGH cardinality]\n",
      "  Unique values: 146\n",
      "  Missing: 0 (0.0%)\n",
      "  Top 5 values:\n",
      "sub_area\n",
      "    Nekrasovka                 1540\n",
      "    Poselenie Sosenskoe        1522\n",
      "    Poselenie Vnukovskoe       1048\n",
      "    Poselenie Moskovskij        807\n",
      "    Poselenie Voskresenskoe     693\n",
      "\n",
      "Encoding Strategy:\n",
      "  LOW cardinality (<10 unique)  → One-Hot Encoding: ['product_type', 'ecology']\n",
      "  HIGH cardinality (≥10 unique) → Target/Frequency Encoding: ['sub_area']\n"
     ]
    }
   ],
   "source": [
    "# Identify categorical columns\n",
    "categorical_cols = data.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"\\nCategorical columns: {len(categorical_cols)}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize lists for cardinality-based categorization\n",
    "low_card = []\n",
    "high_card = []\n",
    "cardinality_threshold = 10\n",
    "\n",
    "for col in categorical_cols:\n",
    "    n_unique = data[col].nunique()\n",
    "    n_missing = data[col].isna().sum()\n",
    "    missing_pct = (n_missing / len(data)) * 100\n",
    "    \n",
    "    # Categorize based on cardinality\n",
    "    if n_unique < cardinality_threshold:\n",
    "        low_card.append(col)\n",
    "        cardinality_label = \"LOW\"\n",
    "    else:\n",
    "        high_card.append(col)\n",
    "        cardinality_label = \"HIGH\"\n",
    "    \n",
    "    print(f\"\\n{col}: [{cardinality_label} cardinality]\")\n",
    "    print(f\"  Unique values: {n_unique}\")\n",
    "    print(f\"  Missing: {n_missing} ({missing_pct:.1f}%)\")\n",
    "    print(f\"  Top 5 values:\")\n",
    "    print(data[col].value_counts().head(5).to_string().replace('\\n', '\\n    '))\n",
    "\n",
    "print(\"\\nEncoding Strategy:\")\n",
    "print(f\"  LOW cardinality (<{cardinality_threshold} unique)  → One-Hot Encoding: {low_card}\")\n",
    "print(f\"  HIGH cardinality (≥{cardinality_threshold} unique) → Target/Frequency Encoding: {high_card}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering (MI-Guided)\n",
    "\n",
    "Compute pairwise mutual information (MI) between features, then programmatically create:\n",
    "- **Ratios & differences** for high MI pairs (>0.5): capture efficiency/gap relationships\n",
    "- **Interactions** for moderate MI pairs (0.1-0.5): capture non-linear dependencies\n",
    "- **Polynomials** (degree 2) for top 5 features with highest MI to target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Created 22 ratio features (MI > 0.5)\n",
      "  Created 22 difference features (MI > 0.5)\n",
      "  Created 54 interaction features (0.1 < MI < 0.5)\n"
     ]
    }
   ],
   "source": [
    "# Calculate Mutual Information matrix for numeric features\n",
    "numeric_cols = data.select_dtypes(include=[np.number]).columns.drop('price_doc')\n",
    "\n",
    "# Compute MI for each pair of features\n",
    "# First, fill NaN values for MI calculation\n",
    "data_filled = data[numeric_cols].fillna(data[numeric_cols].median())\n",
    "\n",
    "# Create MI matrix (pairwise MI between features)\n",
    "mi_matrix = pd.DataFrame(index=numeric_cols, columns=numeric_cols, dtype=float)\n",
    "\n",
    "for i, col1 in enumerate(numeric_cols):\n",
    "    for j, col2 in enumerate(numeric_cols):\n",
    "        if i == j:\n",
    "            mi_matrix.loc[col1, col2] = 0.0\n",
    "        elif i < j:\n",
    "            # Calculate MI between the two features\n",
    "            mi_val = mutual_info_regression(\n",
    "                data_filled[[col1]].values,\n",
    "                data_filled[col2].values,\n",
    "                random_state=42\n",
    "            )[0]\n",
    "            mi_matrix.loc[col1, col2] = mi_val\n",
    "            mi_matrix.loc[col2, col1] = mi_val\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Storage for new features\n",
    "ratio_features = {}\n",
    "diff_features = {}\n",
    "interaction_features = {}\n",
    "\n",
    "# Analyze MI and create features programmatically\n",
    "feature_count = {'ratios': 0, 'differences': 0, 'interactions': 0}\n",
    "\n",
    "for i in range(len(numeric_cols)):\n",
    "    for j in range(i+1, len(numeric_cols)):\n",
    "        col1, col2 = numeric_cols[i], numeric_cols[j]\n",
    "        mi_val = mi_matrix.loc[col1, col2]\n",
    "\n",
    "        # High MI (> 0.5): Create ratios and differences\n",
    "        if mi_val > 0.5:\n",
    "            # Ratio (efficiency/gap measure)\n",
    "            ratio_name = f'{col1}_div_{col2}'\n",
    "            ratio_features[ratio_name] = data[col1] / (data[col2] + 1)\n",
    "\n",
    "            # Difference (gap measure)\n",
    "            diff_name = f'{col1}_minus_{col2}'\n",
    "            diff_features[diff_name] = data[col1] - data[col2]\n",
    "\n",
    "            feature_count['ratios'] += 1\n",
    "            feature_count['differences'] += 1\n",
    "\n",
    "        # Moderate MI (0.1 < MI < 0.5): Create interactions\n",
    "        elif 0.1 < mi_val < 0.5:\n",
    "            interaction_name = f'{col1}_x_{col2}'\n",
    "            interaction_features[interaction_name] = data[col1] * data[col2]\n",
    "\n",
    "            feature_count['interactions'] += 1\n",
    "\n",
    "# Add all programmatic features\n",
    "for name, values in ratio_features.items():\n",
    "    data[name] = values\n",
    "for name, values in diff_features.items():\n",
    "    data[name] = values\n",
    "for name, values in interaction_features.items():\n",
    "    data[name] = values\n",
    "\n",
    "print(f\"  Created {feature_count['ratios']} ratio features (MI > 0.5)\")\n",
    "print(f\"  Created {feature_count['differences']} difference features (MI > 0.5)\")\n",
    "print(f\"  Created {feature_count['interactions']} interaction features (0.1 < MI < 0.5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Created 5 polynomial features (degree 2 for top MI with target)\n"
     ]
    }
   ],
   "source": [
    "# Select key features for polynomial expansion based on MI with target\n",
    "numeric_cols_updated = data.select_dtypes(include=[np.number]).columns.drop('price_doc')\n",
    "data_filled_updated = data[numeric_cols_updated].fillna(data[numeric_cols_updated].median())\n",
    "\n",
    "# Calculate MI with target for all features\n",
    "target_mi = mutual_info_regression(\n",
    "    data_filled_updated.values,\n",
    "    data['price_doc'].values,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create series and sort\n",
    "mi_series = pd.Series(target_mi, index=numeric_cols_updated).sort_values(ascending=False)\n",
    "\n",
    "# Top 5 highest MI features get polynomial (degree 2)\n",
    "top_features = mi_series.head(5).index.tolist()\n",
    "poly_count = 0\n",
    "\n",
    "for feat in top_features:\n",
    "    if feat in data.columns:\n",
    "        data[f'{feat}_squared'] = data[feat] ** 2\n",
    "        poly_count += 1\n",
    "\n",
    "print(f\"  Created {poly_count} polynomial features (degree 2 for top MI with target)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SNR Analysis (Signal-to-Noise Ratio)\n",
    "\n",
    "Calculate SNR = |correlation_with_target| / coefficient_of_variation for each feature. Remove features with SNR < 0.005 (high noise relative to predictive signal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Removed 35 low-SNR features\n"
     ]
    }
   ],
   "source": [
    "# Measure Signal-to-Noise Ratio for ALL features (original + engineered)\n",
    "numeric_cols = data.select_dtypes(include=[np.number]).columns.drop('price_doc')\n",
    "\n",
    "snr_results = []\n",
    "for col in numeric_cols:\n",
    "    valid_mask = data[col].notna()\n",
    "    if valid_mask.sum() > 100:\n",
    "        values = data.loc[valid_mask, col]\n",
    "        target = data.loc[valid_mask, 'price_doc']\n",
    "\n",
    "        # Calculate correlation as signal\n",
    "        corr, _ = stats.pearsonr(values, target)\n",
    "        signal = abs(corr)\n",
    "\n",
    "        # Standard deviation as noise\n",
    "        noise = values.std() / values.mean() if values.mean() != 0 else np.inf\n",
    "\n",
    "        # SNR\n",
    "        snr = signal / noise if noise > 0 else 0\n",
    "\n",
    "        snr_results.append({\n",
    "            'feature': col,\n",
    "            'signal': signal,\n",
    "            'noise': noise,\n",
    "            'snr': snr\n",
    "        })\n",
    "\n",
    "snr_df = pd.DataFrame(snr_results).sort_values('snr', ascending=False)\n",
    "\n",
    "# Remove low SNR features\n",
    "low_snr = snr_df[snr_df['snr'] < 0.005]['feature'].tolist()\n",
    "if low_snr:\n",
    "    data = data.drop(columns=low_snr)\n",
    "    print(f\"  Removed {len(low_snr)} low-SNR features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIF Analysis (Multicollinearity Removal)\n",
    "\n",
    "Identify features with both high correlation (Pearson > 0.95) AND high VIF (>10). Only drop features meeting both criteria to remove redundancy while preserving complementary information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Removed 17 features (high corr AND high VIF)\n"
     ]
    }
   ],
   "source": [
    "# Identify highly correlated features (Pearson > 0.95)\n",
    "numeric_cols = data.select_dtypes(include=[np.number]).columns.drop('price_doc')\n",
    "corr_matrix = data[numeric_cols].corr().abs()\n",
    "\n",
    "# Get upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find features with correlation > 0.95\n",
    "to_drop_corr = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "# Calculate VIF for each feature\n",
    "X_vif = data[numeric_cols].fillna(data[numeric_cols].median())\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X_vif.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_vif.values, i) for i in range(len(X_vif.columns))]\n",
    "\n",
    "# Identify features with VIF > 10\n",
    "high_vif = vif_data[vif_data['VIF'] > 10]['feature'].tolist()\n",
    "\n",
    "# Only drop features that are in BOTH lists (high correlation AND high VIF)\n",
    "to_drop_multicollinear = list(set(to_drop_corr).intersection(set(high_vif)))\n",
    "\n",
    "if to_drop_multicollinear:\n",
    "    data = data.drop(columns=to_drop_multicollinear)\n",
    "    print(f\"  Removed {len(to_drop_multicollinear)} features (high corr AND high VIF)\")\n",
    "else:\n",
    "    print(f\"  No features with both high correlation AND high VIF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data (do this BEFORE encoding to preserve categorical columns)\n",
    "X = data.drop('price_doc', axis=1)\n",
    "y = data['price_doc']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save original split with categorical columns for encoding later\n",
    "X_train_orig = X_train.copy()\n",
    "X_test_orig = X_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Imputation Strategy Selection\n",
    "\n",
    "Test 4 imputation methods (median, mean, KNN-5, KNN-10) using Ridge regression. Select strategy with lowest test RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing imputation strategies...\n",
      "  median      : RMSE = 0.4984\n",
      "  mean        : RMSE = 0.5030\n",
      "  knn_5       : RMSE = 0.4855\n",
      "  knn_10      : RMSE = 0.4854\n",
      "\n",
      "  Selected best: knn_10 (RMSE: 0.4854)\n"
     ]
    }
   ],
   "source": [
    "# Test multiple imputation methods and select best\n",
    "print(\"Testing imputation strategies...\")\n",
    "\n",
    "numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "imputation_strategies = {\n",
    "    'median': SimpleImputer(strategy='median'),\n",
    "    'mean': SimpleImputer(strategy='mean'),\n",
    "    'knn_5': KNNImputer(n_neighbors=5),\n",
    "    'knn_10': KNNImputer(n_neighbors=10)\n",
    "}\n",
    "\n",
    "imputation_results = []\n",
    "\n",
    "for strategy_name, imputer in imputation_strategies.items():\n",
    "    # Create temporary imputed datasets\n",
    "    X_train_temp = imputer.fit_transform(X_train[numeric_features])\n",
    "    X_test_temp = imputer.transform(X_test[numeric_features])\n",
    "\n",
    "    X_train_temp_df = pd.DataFrame(X_train_temp, columns=numeric_features, index=X_train.index)\n",
    "    X_test_temp_df = pd.DataFrame(X_test_temp, columns=numeric_features, index=X_test.index)\n",
    "\n",
    "    # Quick model to evaluate\n",
    "    scaler_temp = RobustScaler()\n",
    "    X_train_scaled = scaler_temp.fit_transform(X_train_temp_df)\n",
    "    X_test_scaled = scaler_temp.transform(X_test_temp_df)\n",
    "\n",
    "    model_temp = Ridge(alpha=1.0, random_state=42)\n",
    "    model_temp.fit(X_train_scaled, y_train)\n",
    "\n",
    "    y_pred = model_temp.predict(X_test_scaled)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "    imputation_results.append({\n",
    "        'strategy': strategy_name,\n",
    "        'rmse': rmse\n",
    "    })\n",
    "    print(f\"  {strategy_name:12s}: RMSE = {rmse:.4f}\")\n",
    "\n",
    "# Select best imputation strategy\n",
    "best_strategy = min(imputation_results, key=lambda x: x['rmse'])\n",
    "print(f\"\\n  Selected best: {best_strategy['strategy']} (RMSE: {best_strategy['rmse']:.4f})\")\n",
    "\n",
    "# Apply best imputation\n",
    "best_imputer = imputation_strategies[best_strategy['strategy']]\n",
    "X_train_imputed = best_imputer.fit_transform(X_train[numeric_features])\n",
    "X_test_imputed = best_imputer.transform(X_test[numeric_features])\n",
    "\n",
    "X_train = pd.DataFrame(X_train_imputed, columns=numeric_features, index=X_train.index)\n",
    "X_test = pd.DataFrame(X_test_imputed, columns=numeric_features, index=X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Outliers clipped at 1st and 99th percentiles\n"
     ]
    }
   ],
   "source": [
    "# Clip outliers at 1st and 99th percentiles\n",
    "for col in X_train.columns:\n",
    "    if X_train[col].dtype in [np.float64, np.int64]:\n",
    "        lower = X_train[col].quantile(0.01)\n",
    "        upper = X_train[col].quantile(0.99)\n",
    "        X_train[col] = X_train[col].clip(lower, upper)\n",
    "        X_test[col] = X_test[col].clip(lower, upper)\n",
    "\n",
    "print(\"  Outliers clipped at 1st and 99th percentiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding (Low Cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  One-hot encoded 2 low-cardinality features: ['product_type', 'ecology']\n"
     ]
    }
   ],
   "source": [
    "# Use the low_card list from categorical overview section\n",
    "for col in low_card:\n",
    "    if col in X_train.columns:\n",
    "        # One-hot encode for train\n",
    "        train_dummies = pd.get_dummies(X_train[col], prefix=col, drop_first=True)\n",
    "        X_train = pd.concat([X_train.drop(col, axis=1), train_dummies], axis=1)\n",
    "\n",
    "        # One-hot encode for test\n",
    "        test_dummies = pd.get_dummies(X_test[col], prefix=col, drop_first=True)\n",
    "        X_test = pd.concat([X_test.drop(col, axis=1), test_dummies], axis=1)\n",
    "\n",
    "        # Align columns\n",
    "        X_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)\n",
    "\n",
    "print(f\"  One-hot encoded {len([c for c in low_card if c in X_train_orig.columns])} low-cardinality features: {[c for c in low_card if c in X_train_orig.columns]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Encoding Strategy Selection (High Cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing encoding strategies...\n",
      "  target_encoding     : RMSE = 0.4669\n",
      "  frequency_encoding  : RMSE = 0.4766\n",
      "\n",
      "  Selected best: target_encoding (RMSE: 0.4669)\n",
      "  Applied target_encoding for 1 high-cardinality features: ['sub_area']\n"
     ]
    }
   ],
   "source": [
    "# Test target encoding vs frequency encoding, select best\n",
    "print(\"Testing encoding strategies...\")\n",
    "\n",
    "# Use the high_card list from categorical overview section\n",
    "smoothing = 10\n",
    "\n",
    "encoding_strategies = {\n",
    "    'target_encoding': {},\n",
    "    'frequency_encoding': {}\n",
    "}\n",
    "\n",
    "# Test both encoding strategies\n",
    "for strategy_name in encoding_strategies.keys():\n",
    "    X_train_temp = X_train.copy()\n",
    "    X_test_temp = X_test.copy()\n",
    "\n",
    "    for col in high_card:\n",
    "        if col in X_train_orig.columns:\n",
    "            if strategy_name == 'target_encoding':\n",
    "                # Target encoding with K-Fold\n",
    "                kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "                train_encoded = np.zeros(len(X_train_orig))\n",
    "                global_mean = y_train.mean()\n",
    "\n",
    "                for train_idx, val_idx in kf.split(X_train_orig):\n",
    "                    X_fold = X_train_orig.iloc[train_idx]\n",
    "                    y_fold = y_train.iloc[train_idx]\n",
    "\n",
    "                    temp_df = pd.DataFrame({col: X_fold[col], 'target': y_fold.values})\n",
    "                    means = temp_df.groupby(col)['target'].agg(['mean', 'count'])\n",
    "                    smoothed = (means['mean'] * means['count'] + global_mean * smoothing) / (means['count'] + smoothing)\n",
    "\n",
    "                    train_encoded[val_idx] = X_train_orig.iloc[val_idx][col].map(smoothed).fillna(global_mean)\n",
    "\n",
    "                temp_df = pd.DataFrame({col: X_train_orig[col], 'target': y_train.values})\n",
    "                means = temp_df.groupby(col)['target'].agg(['mean', 'count'])\n",
    "                smoothed = (means['mean'] * means['count'] + global_mean * smoothing) / (means['count'] + smoothing)\n",
    "                test_encoded = X_test_orig[col].map(smoothed).fillna(global_mean)\n",
    "\n",
    "                X_train_temp[f'{col}_enc'] = train_encoded\n",
    "                X_test_temp[f'{col}_enc'] = test_encoded\n",
    "\n",
    "            else:  # frequency_encoding\n",
    "                # Frequency encoding\n",
    "                freq_map = X_train_orig[col].value_counts(normalize=True).to_dict()\n",
    "                X_train_temp[f'{col}_enc'] = X_train_orig[col].map(freq_map).fillna(0)\n",
    "                X_test_temp[f'{col}_enc'] = X_test_orig[col].map(freq_map).fillna(0)\n",
    "\n",
    "    # Quick model to evaluate\n",
    "    scaler_temp = RobustScaler()\n",
    "    X_train_scaled = scaler_temp.fit_transform(X_train_temp)\n",
    "    X_test_scaled = scaler_temp.transform(X_test_temp)\n",
    "\n",
    "    model_temp = Ridge(alpha=1.0, random_state=42)\n",
    "    model_temp.fit(X_train_scaled, y_train)\n",
    "\n",
    "    y_pred = model_temp.predict(X_test_scaled)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "    encoding_strategies[strategy_name]['rmse'] = rmse\n",
    "    encoding_strategies[strategy_name]['X_train'] = X_train_temp\n",
    "    encoding_strategies[strategy_name]['X_test'] = X_test_temp\n",
    "\n",
    "    print(f\"  {strategy_name:20s}: RMSE = {rmse:.4f}\")\n",
    "\n",
    "# Select best encoding strategy\n",
    "best_encoding = min(encoding_strategies.items(), key=lambda x: x[1]['rmse'])\n",
    "print(f\"\\n  Selected best: {best_encoding[0]} (RMSE: {best_encoding[1]['rmse']:.4f})\")\n",
    "\n",
    "# Apply best encoding\n",
    "X_train = best_encoding[1]['X_train']\n",
    "X_test = best_encoding[1]['X_test']\n",
    "\n",
    "encoded_high_card = [c for c in high_card if c in X_train_orig.columns]\n",
    "print(f\"  Applied {best_encoding[0]} for {len(encoded_high_card)} high-cardinality features: {encoded_high_card}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Removal via Cook's Distance\n",
    "\n",
    "Iteratively remove influential outliers using Cook's Distance with a conservative threshold to preserve data while removing extreme leverage points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training samples: 21600\n",
      "Cook's Distance threshold: 0.000741\n",
      "  Iteration 1: Removed 198 outliers (Cook's D > 0.000741)\n",
      "  Iteration 2: Removed 84 outliers (Cook's D > 0.000741)\n",
      "  Iteration 3: Removed 42 outliers (Cook's D > 0.000741)\n",
      "\n",
      "Total outliers removed: 324\n",
      "Final training samples: 21276 (98.50% retained)\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Conservative Cook's Distance threshold (4/n is standard, we use 4*4/n for conservativeness)\n",
    "n = len(X_train)\n",
    "p = X_train.shape[1]\n",
    "cook_threshold = 4 * (4 / n)  # Very conservative: 16/n instead of 4/n\n",
    "\n",
    "print(f\"Initial training samples: {len(X_train)}\")\n",
    "print(f\"Cook's Distance threshold: {cook_threshold:.6f}\")\n",
    "\n",
    "max_iterations = 3  # Limit iterations to prevent over-removal\n",
    "iteration = 0\n",
    "outliers_removed_total = 0\n",
    "\n",
    "while iteration < max_iterations:\n",
    "    iteration += 1\n",
    "    \n",
    "    # Fit OLS model to calculate Cook's Distance\n",
    "    X_train_with_const = sm.add_constant(X_train)\n",
    "    model_ols = sm.OLS(y_train, X_train_with_const).fit()\n",
    "    \n",
    "    # Calculate Cook's Distance\n",
    "    influence = OLSInfluence(model_ols)\n",
    "    cooks_d = influence.cooks_distance[0]\n",
    "    \n",
    "    # Identify outliers\n",
    "    outlier_mask = cooks_d > cook_threshold\n",
    "    n_outliers = outlier_mask.sum()\n",
    "    \n",
    "    if n_outliers == 0:\n",
    "        print(f\"  Iteration {iteration}: No outliers found, stopping\")\n",
    "        break\n",
    "    \n",
    "    # Remove outliers from training data\n",
    "    X_train = X_train[~outlier_mask]\n",
    "    y_train = y_train[~outlier_mask]\n",
    "    outliers_removed_total += n_outliers\n",
    "    \n",
    "    print(f\"  Iteration {iteration}: Removed {n_outliers} outliers (Cook's D > {cook_threshold:.6f})\")\n",
    "    \n",
    "    # Recalculate n for next iteration\n",
    "    n = len(X_train)\n",
    "\n",
    "print(f\"\\nTotal outliers removed: {outliers_removed_total}\")\n",
    "print(f\"Final training samples: {len(X_train)} ({100 * len(X_train) / (len(X_train) + outliers_removed_total):.2f}% retained)\")\n",
    "\n",
    "# Note: X_test remains unchanged (we don't remove test outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Robust scaling applied\n"
     ]
    }
   ],
   "source": [
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_final = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_final = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(f\"  Robust scaling applied\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ridge(alpha=1.0, random_state=42)\n",
    "model.fit(X_train_final, y_train)\n",
    "\n",
    "# Predictions\n",
    "train_pred = model.predict(X_train_final)\n",
    "test_pred = model.predict(X_test_final)\n",
    "\n",
    "# Metrics\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "test_r2 = r2_score(y_test, test_pred)\n",
    "\n",
    "# Convert to RUB\n",
    "test_pred_rub = np.expm1(test_pred)\n",
    "y_test_rub = np.expm1(y_test)\n",
    "test_rmse_rub = np.sqrt(mean_squared_error(y_test_rub, test_pred_rub))\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(model, X_train_final, y_train, cv=5,\n",
    "                            scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "cv_rmse = np.sqrt(-cv_scores).mean()\n",
    "cv_rmse_std = np.sqrt(-cv_scores).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RESULTS\n",
      "================================================================================\n",
      "Features:       66\n",
      "Samples:        21276\n",
      "\n",
      "Test RMSE:      0.4681 (log) | 3,096,621 RUB\n",
      "5-Fold CV RMSE MEAN: 0.4473 (log)\n",
      "5-Fold CV RMSE STD: 0.0084 (log)\n",
      "\n",
      "Baseline RMSE:  5,626,709 RUB\n",
      "Improvement:    +44.97%\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Features:       {X_train_final.shape[1]}\")\n",
    "print(f\"Samples:        {len(y_train)}\")\n",
    "print(f\"\\nTest RMSE:      {test_rmse:.4f} (log) | {test_rmse_rub:,.0f} RUB\")\n",
    "print(f\"5-Fold CV RMSE MEAN: {cv_rmse:.4f} (log)\")\n",
    "print(f\"5-Fold CV RMSE STD: {cv_rmse_std:.4f} (log)\")\n",
    "\n",
    "# Compare with baseline\n",
    "try:\n",
    "    with open(\"baseline_rmse.txt\", \"r\") as f:\n",
    "        baseline_rmse_rub = float(f.read().strip())\n",
    "    improvement = ((baseline_rmse_rub - test_rmse_rub) / baseline_rmse_rub) * 100\n",
    "    print(f\"\\nBaseline RMSE:  {baseline_rmse_rub:,.0f} RUB\")\n",
    "    print(f\"Improvement:    {improvement:+.2f}%\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Calculate residuals\n",
    "# residuals = y_test - test_pred\n",
    "\n",
    "# # 1. Check homoscedasticity\n",
    "# plt.figure(figsize=(12, 5))\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.scatter(test_pred, residuals, alpha=0.6)\n",
    "# plt.xlabel(\"Fitted Values (Predictions)\")\n",
    "# plt.ylabel(\"Residuals\")\n",
    "# plt.title(\"Residuals vs Fitted\")\n",
    "# plt.axhline(y=0, color='red', linestyle='--')\n",
    "\n",
    "# # 2. Check normality of residuals\n",
    "# plt.subplot(1, 2, 2)\n",
    "# stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "# plt.title(\"Q-Q Plot - Residual Normality\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TECHNIQUES APPLIED:\n",
    "1. Programmatic feature engineering (MI-guided):\n",
    "   - Ratios/differences for MI > 0.5 (efficiency measures)\n",
    "   - Interactions for 0.1 < MI < 0.5 (non-linear relationships)\n",
    "   - Polynomial features (degree 2 for top MI with target)\n",
    "\n",
    "2. SNR analysis (signal-to-noise ratio screening)\n",
    "3. Multicollinearity removal (Pearson r > 0.95 AND VIF > 10)\n",
    "4. Adaptive imputation (tested 4 strategies, selected best)\n",
    "5. Adaptive encoding (target vs frequency, selected best)\n",
    "6. Outlier clipping (1st and 99th percentiles)\n",
    "7. Cook's Distance outlier removal (conservative iterative approach, threshold=16/n)\n",
    "8. Robust scaling (IQR-based)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
