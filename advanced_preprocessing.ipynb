{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sberbank Housing - Advanced Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor, OLSInfluence\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data & Log-Transform Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 27000 samples, 18 features\n",
      "  Target log-transformed\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data = pd.read_csv(r\"data/sberbank_housing.csv\", index_col=0, low_memory=False)\n",
    "data.columns = [c.lower().strip().replace(\" \", \"_\") for c in data.columns]\n",
    "data = data.drop(columns=[\"timestamp\", \"id\"])\n",
    "\n",
    "print(f\"Original: {data.shape[0]} samples, {data.shape[1]} features\")\n",
    "\n",
    "# Log-transform target\n",
    "data['price_doc'] = np.log1p(data['price_doc'])\n",
    "print(\"  Target log-transformed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Features Overview\n",
    "\n",
    "Examine categorical columns to determine encoding strategy based on cardinality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Categorical columns: 3\n",
      "============================================================\n",
      "\n",
      "product_type: [LOW cardinality]\n",
      "  Unique values: 2\n",
      "  Missing: 0 (0.0%)\n",
      "  Top 5 values:\n",
      "product_type\n",
      "    Investment       17542\n",
      "    OwnerOccupier     9458\n",
      "\n",
      "ecology: [LOW cardinality]\n",
      "  Unique values: 5\n",
      "  Missing: 0 (0.0%)\n",
      "  Top 5 values:\n",
      "ecology\n",
      "    poor            7215\n",
      "    no data         6570\n",
      "    good            6548\n",
      "    excellent       3419\n",
      "    satisfactory    3248\n",
      "\n",
      "sub_area: [HIGH cardinality]\n",
      "  Unique values: 146\n",
      "  Missing: 0 (0.0%)\n",
      "  Top 5 values:\n",
      "sub_area\n",
      "    Nekrasovka                 1540\n",
      "    Poselenie Sosenskoe        1522\n",
      "    Poselenie Vnukovskoe       1048\n",
      "    Poselenie Moskovskij        807\n",
      "    Poselenie Voskresenskoe     693\n",
      "\n",
      "Encoding Strategy:\n",
      "  LOW cardinality (<10 unique)  → One-Hot Encoding: ['product_type', 'ecology']\n",
      "  HIGH cardinality (≥10 unique) → Target/Frequency Encoding: ['sub_area']\n"
     ]
    }
   ],
   "source": [
    "# Identify categorical columns\n",
    "categorical_cols = data.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"\\nCategorical columns: {len(categorical_cols)}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize lists for cardinality-based categorization\n",
    "low_card = []\n",
    "high_card = []\n",
    "cardinality_threshold = 10\n",
    "\n",
    "for col in categorical_cols:\n",
    "    n_unique = data[col].nunique()\n",
    "    n_missing = data[col].isna().sum()\n",
    "    missing_pct = (n_missing / len(data)) * 100\n",
    "    \n",
    "    # Categorize based on cardinality\n",
    "    if n_unique < cardinality_threshold:\n",
    "        low_card.append(col)\n",
    "        cardinality_label = \"LOW\"\n",
    "    else:\n",
    "        high_card.append(col)\n",
    "        cardinality_label = \"HIGH\"\n",
    "    \n",
    "    print(f\"\\n{col}: [{cardinality_label} cardinality]\")\n",
    "    print(f\"  Unique values: {n_unique}\")\n",
    "    print(f\"  Missing: {n_missing} ({missing_pct:.1f}%)\")\n",
    "    print(f\"  Top 5 values:\")\n",
    "    print(data[col].value_counts().head(5).to_string().replace('\\n', '\\n    '))\n",
    "\n",
    "print(\"\\nEncoding Strategy:\")\n",
    "print(f\"  LOW cardinality (<{cardinality_threshold} unique)  → One-Hot Encoding: {low_card}\")\n",
    "print(f\"  HIGH cardinality (≥{cardinality_threshold} unique) → Target/Frequency Encoding: {high_card}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering (MI-Guided)\n",
    "\n",
    "Compute pairwise mutual information (MI) between features, then programmatically create:\n",
    "- **Ratios & differences** for high MI pairs (>0.5 nats): capture relative scale and gap relationships between related features\n",
    "- **Interactions** for moderate MI pairs (0.1-0.5 nats): capture multiplicative dependencies\n",
    "- **Polynomials** (degree 2) for top 5 features with highest MI to target: capture non-linear relationships\n",
    "\n",
    "MI measures information one feature provides about another. Higher MI = stronger relationship.\n",
    "MI in nats (sklearn uses natural log): 0 = independent, >0.5 = strong dependence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Created 22 ratio features (MI > 0.5)\n",
      "  Created 22 difference features (MI > 0.5)\n",
      "  Created 54 interaction features (0.1 < MI < 0.5)\n"
     ]
    }
   ],
   "source": [
    "# Calculate Mutual Information matrix for numeric features\n",
    "numeric_cols = data.select_dtypes(include=[np.number]).columns.drop('price_doc')\n",
    "\n",
    "# Compute MI for each pair of features\n",
    "# First, fill NaN values for MI calculation\n",
    "data_filled = data[numeric_cols].fillna(data[numeric_cols].median())\n",
    "\n",
    "# Create MI matrix (pairwise MI between features)\n",
    "mi_matrix = pd.DataFrame(index=numeric_cols, columns=numeric_cols, dtype=float)\n",
    "\n",
    "for i, col1 in enumerate(numeric_cols):\n",
    "    for j, col2 in enumerate(numeric_cols):\n",
    "        if i == j:\n",
    "            mi_matrix.loc[col1, col2] = 0.0\n",
    "        elif i < j:\n",
    "            # Calculate MI between the two features\n",
    "            mi_val = mutual_info_regression(\n",
    "                data_filled[[col1]].values,\n",
    "                data_filled[col2].values,\n",
    "                random_state=42\n",
    "            )[0]\n",
    "            mi_matrix.loc[col1, col2] = mi_val\n",
    "            mi_matrix.loc[col2, col1] = mi_val\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Storage for new features\n",
    "ratio_features = {}\n",
    "diff_features = {}\n",
    "interaction_features = {}\n",
    "\n",
    "# Analyze MI and create features programmatically\n",
    "feature_count = {'ratios': 0, 'differences': 0, 'interactions': 0}\n",
    "\n",
    "for i in range(len(numeric_cols)):\n",
    "    for j in range(i+1, len(numeric_cols)):\n",
    "        col1, col2 = numeric_cols[i], numeric_cols[j]\n",
    "        mi_val = mi_matrix.loc[col1, col2]\n",
    "\n",
    "        # nats = bits * log(2) ~ bits * 0.693 (from sklearn's implementation). Rules are derived from common practices by the practitioners, not strict\n",
    "        # High MI (> 0.5 nats): Create ratios and differences\n",
    "        if mi_val > 0.5:\n",
    "            # Ratio (efficiency/gap measure)\n",
    "            ratio_name = f'{col1}_div_{col2}'\n",
    "            ratio_features[ratio_name] = data[col1] / (data[col2] + 1)\n",
    "\n",
    "            # Difference (gap measure)\n",
    "            diff_name = f'{col1}_minus_{col2}'\n",
    "            diff_features[diff_name] = data[col1] - data[col2]\n",
    "\n",
    "            feature_count['ratios'] += 1\n",
    "            feature_count['differences'] += 1\n",
    "\n",
    "        # Moderate MI (0.1 nats < MI < 0.5 nats): Create interactions\n",
    "        elif 0.1 < mi_val < 0.5:\n",
    "            interaction_name = f'{col1}_x_{col2}'\n",
    "            interaction_features[interaction_name] = data[col1] * data[col2]\n",
    "\n",
    "            feature_count['interactions'] += 1\n",
    "\n",
    "# Add all programmatic features\n",
    "for name, values in ratio_features.items():\n",
    "    data[name] = values\n",
    "for name, values in diff_features.items():\n",
    "    data[name] = values\n",
    "for name, values in interaction_features.items():\n",
    "    data[name] = values\n",
    "\n",
    "print(f\"  Created {feature_count['ratios']} ratio features (MI > 0.5)\")\n",
    "print(f\"  Created {feature_count['differences']} difference features (MI > 0.5)\")\n",
    "print(f\"  Created {feature_count['interactions']} interaction features (0.1 < MI < 0.5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Features\n",
    "\n",
    "Purpose: Capture non-linear relationships for the most predictive features\n",
    "\n",
    "Mutual Information (MI) between each feature and target --> select top 5 features with highest MI --> create squared terms (degree 2) for these features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Created 5 polynomial features (degree 2 for top MI with target)\n"
     ]
    }
   ],
   "source": [
    "# Select key features for polynomial expansion based on MI with target\n",
    "numeric_cols_updated = data.select_dtypes(include=[np.number]).columns.drop('price_doc')\n",
    "data_filled_updated = data[numeric_cols_updated].fillna(data[numeric_cols_updated].median())\n",
    "\n",
    "# Calculate MI with target for all features\n",
    "target_mi = mutual_info_regression(\n",
    "    data_filled_updated.values,\n",
    "    data['price_doc'].values,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create series and sort\n",
    "mi_series = pd.Series(target_mi, index=numeric_cols_updated).sort_values(ascending=False)\n",
    "\n",
    "# Top 5 highest MI features get polynomial (degree 2)\n",
    "top_features = mi_series.head(5).index.tolist()\n",
    "poly_count = 0\n",
    "\n",
    "for feat in top_features:\n",
    "    if feat in data.columns:\n",
    "        data[f'{feat}_squared'] = data[feat] ** 2\n",
    "        poly_count += 1\n",
    "\n",
    "print(f\"  Created {poly_count} polynomial features (degree 2 for top MI with target)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIF Analysis (Multicollinearity Removal)\n",
    "\n",
    "Identify features with both high correlation (Pearson > 0.95) AND high VIF (>10). Iteratively remove features with high VIF (>10). When highly correlated with others, drop the one with lower target correlation (keep the more predictive one).\n",
    "\n",
    "Variance Inflation Factor - measures how much a feature can be predicted by other features\n",
    "\n",
    "VIF = 1 / (1 - R²)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Removed 50 features with high VIF (iterative): ['raion_popul_div_kindergarten_km', 'raion_popul_div_school_km', 'raion_popul', 'build_year_x_park_km', 'build_year_x_railroad_km', 'max_floor_minus_build_year', 'kitch_sq_minus_build_year', 'build_year', 'build_year_minus_kindergarten_km', 'raion_popul_minus_railroad_km', 'raion_popul_minus_kindergarten_km', 'raion_popul_minus_railroad_km_squared', 'raion_popul_minus_school_km_squared', 'raion_popul_minus_school_km', 'build_year_minus_school_km', 'build_year_x_state', 'build_year_div_raion_popul', 'build_year_div_kindergarten_km', 'build_year_div_school_km', 'full_sq_x_build_year', 'build_year_x_num_room', 'railroad_km_minus_metro_min_walk', 'raion_popul_minus_park_km', 'school_km_minus_metro_min_walk', 'park_km_minus_metro_min_walk', 'full_sq_x_raion_popul_squared', 'school_km', 'park_km', 'park_km_minus_railroad_km', 'kindergarten_km_minus_park_km', 'kindergarten_km_minus_railroad_km', 'school_km_minus_railroad_km', 'kindergarten_km_minus_metro_min_walk', 'kitch_sq_minus_max_floor', 'life_sq_x_build_year', 'kitch_sq', 'kitch_sq_x_school_km', 'life_sq_x_kitch_sq', 'kitch_sq_x_park_km', 'full_sq_x_park_km', 'kitch_sq_x_num_room', 'full_sq_x_school_km', 'life_sq_x_max_floor', 'life_sq', 'kitch_sq_x_state', 'metro_min_walk', 'build_year_x_metro_min_walk', 'kindergarten_km_div_railroad_km', 'kitch_sq_x_kindergarten_km', 'life_sq_x_park_km']\n"
     ]
    }
   ],
   "source": [
    "# Calculate correlation with target once (for deciding which feature to keep)\n",
    "numeric_cols = data.select_dtypes(include=[np.number]).columns.drop('price_doc')\n",
    "target_corr = data[numeric_cols].corrwith(data['price_doc']).abs()\n",
    "\n",
    "# Iterative VIF removal\n",
    "dropped_features = []\n",
    "max_iterations = 50  # Prevent infinite loops\n",
    "\n",
    "for iteration in range(max_iterations):\n",
    "    # Get current numeric columns\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns.drop('price_doc')\n",
    "    \n",
    "    if len(numeric_cols) < 2:\n",
    "        break\n",
    "    \n",
    "    # Calculate VIF - inflation of the variance of a feature's regression coefficient\n",
    "    X_vif = data[numeric_cols].fillna(data[numeric_cols].median())\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = X_vif.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X_vif.values, i) for i in range(len(X_vif.columns))]\n",
    "    \n",
    "    # Find highest VIF\n",
    "    max_vif_row = vif_data.loc[vif_data['VIF'].idxmax()]\n",
    "    \n",
    "    if max_vif_row['VIF'] > 10:\n",
    "        feat_to_drop = max_vif_row['feature']\n",
    "        \n",
    "        # Check if it's highly correlated with another feature\n",
    "        corr_matrix = data[numeric_cols].corr().abs()\n",
    "        highly_correlated = corr_matrix[feat_to_drop][corr_matrix[feat_to_drop] > 0.95].index.tolist()\n",
    "        highly_correlated.remove(feat_to_drop)  # Remove self\n",
    "        \n",
    "        # If correlated with others, drop the one with LOWER target correlation\n",
    "        if highly_correlated:\n",
    "            candidates = highly_correlated + [feat_to_drop]\n",
    "            # Drop the one with lowest correlation to target\n",
    "            feat_to_drop = min(candidates, key=lambda x: target_corr.get(x, 0))\n",
    "        \n",
    "        # Drop the feature\n",
    "        data = data.drop(columns=[feat_to_drop])\n",
    "        dropped_features.append(feat_to_drop)\n",
    "    else:\n",
    "        # No more high VIF features\n",
    "        break\n",
    "\n",
    "if dropped_features:\n",
    "    print(f\"  Removed {len(dropped_features)} features with high VIF (iterative): {dropped_features}\")\n",
    "else:\n",
    "    print(f\"  No features with high VIF removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data (do this BEFORE encoding to preserve categorical columns)\n",
    "X = data.drop('price_doc', axis=1)\n",
    "y = data['price_doc']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save original split with categorical columns for encoding later\n",
    "X_train_orig = X_train.copy()\n",
    "X_test_orig = X_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Imputation Strategy Selection\n",
    "\n",
    "Test 4 imputation methods (median, mean, KNN-5, KNN-10) using Ridge regression. Select strategy with lowest test RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing imputation strategies...\n",
      "  median      : RMSE = 0.4931\n",
      "  mean        : RMSE = 0.4965\n",
      "  knn_5       : RMSE = 0.4902\n",
      "  knn_10      : RMSE = 0.4901\n",
      "\n",
      "  Selected best: knn_10 (RMSE: 0.4901)\n"
     ]
    }
   ],
   "source": [
    "# Test multiple imputation methods and select best\n",
    "print(\"Testing imputation strategies...\")\n",
    "\n",
    "numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "imputation_strategies = {\n",
    "    'median': SimpleImputer(strategy='median'),\n",
    "    'mean': SimpleImputer(strategy='mean'),\n",
    "    'knn_5': KNNImputer(n_neighbors=5),\n",
    "    'knn_10': KNNImputer(n_neighbors=10)\n",
    "}\n",
    "\n",
    "imputation_results = []\n",
    "\n",
    "for strategy_name, imputer in imputation_strategies.items():\n",
    "    # Create temporary imputed datasets\n",
    "    X_train_temp = imputer.fit_transform(X_train[numeric_features])\n",
    "    X_test_temp = imputer.transform(X_test[numeric_features])\n",
    "\n",
    "    X_train_temp_df = pd.DataFrame(X_train_temp, columns=numeric_features, index=X_train.index)\n",
    "    X_test_temp_df = pd.DataFrame(X_test_temp, columns=numeric_features, index=X_test.index)\n",
    "\n",
    "    # Quick model to evaluate\n",
    "    scaler_temp = RobustScaler()\n",
    "    X_train_scaled = scaler_temp.fit_transform(X_train_temp_df)\n",
    "    X_test_scaled = scaler_temp.transform(X_test_temp_df)\n",
    "\n",
    "    model_temp = Ridge(alpha=1.0, random_state=42)\n",
    "    model_temp.fit(X_train_scaled, y_train)\n",
    "\n",
    "    y_pred = model_temp.predict(X_test_scaled)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "    imputation_results.append({\n",
    "        'strategy': strategy_name,\n",
    "        'rmse': rmse\n",
    "    })\n",
    "    print(f\"  {strategy_name:12s}: RMSE = {rmse:.4f}\")\n",
    "\n",
    "# Select best imputation strategy\n",
    "best_strategy = min(imputation_results, key=lambda x: x['rmse'])\n",
    "print(f\"\\n  Selected best: {best_strategy['strategy']} (RMSE: {best_strategy['rmse']:.4f})\")\n",
    "\n",
    "# Apply best imputation\n",
    "best_imputer = imputation_strategies[best_strategy['strategy']]\n",
    "X_train_imputed = best_imputer.fit_transform(X_train[numeric_features])\n",
    "X_test_imputed = best_imputer.transform(X_test[numeric_features])\n",
    "\n",
    "X_train = pd.DataFrame(X_train_imputed, columns=numeric_features, index=X_train.index)\n",
    "X_test = pd.DataFrame(X_test_imputed, columns=numeric_features, index=X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding (Low Cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  One-hot encoded 2 low-cardinality features: ['product_type', 'ecology']\n"
     ]
    }
   ],
   "source": [
    "# Use the low_card list from categorical overview section\n",
    "for col in low_card:\n",
    "    if col in X_train.columns:\n",
    "        # One-hot encode for train\n",
    "        train_dummies = pd.get_dummies(X_train[col], prefix=col, drop_first=True)\n",
    "        X_train = pd.concat([X_train.drop(col, axis=1), train_dummies], axis=1)\n",
    "\n",
    "        # One-hot encode for test\n",
    "        test_dummies = pd.get_dummies(X_test[col], prefix=col, drop_first=True)\n",
    "        X_test = pd.concat([X_test.drop(col, axis=1), test_dummies], axis=1)\n",
    "\n",
    "        # Align columns\n",
    "        X_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)\n",
    "\n",
    "print(f\"  One-hot encoded {len([c for c in low_card if c in X_train_orig.columns])} low-cardinality features: {[c for c in low_card if c in X_train_orig.columns]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Encoding Strategy Selection (High Cardinality)\n",
    "\n",
    "For target encoding: K-Fold prevents leakage by ensuring each point's encoding doesn't include its own target\n",
    "\n",
    "Two strategies tested:\n",
    "\n",
    "1. Target Encoding\n",
    "\n",
    "    - Replace category with smoothed mean of target values for that category\n",
    "    - Smoothing formula: (category_mean × count + global_mean × 10) / (count + 10)\n",
    "    - Rare categories → pulled toward global mean (prevents overfitting)\n",
    "    - K-Fold CV (5 folds): Prevents target leakage by encoding each fold using only other folds' statistics\n",
    "    - Unseen categories → filled with global mean\n",
    "\n",
    "2. Frequency Encoding\n",
    "\n",
    "    - Replace category with its frequency (proportion of occurrences)\n",
    "    - Simple, no leakage risk, captures category prevalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing encoding strategies...\n",
      "  target_encoding     : RMSE = 0.4763\n",
      "  frequency_encoding  : RMSE = 0.4900\n",
      "\n",
      "  Selected best: target_encoding (RMSE: 0.4763)\n",
      "  Applied target_encoding for 1 high-cardinality features: ['sub_area']\n"
     ]
    }
   ],
   "source": [
    "# Test target encoding vs frequency encoding, select best\n",
    "print(\"Testing encoding strategies...\")\n",
    "\n",
    "# Use the high_card list from categorical overview section\n",
    "smoothing = 10\n",
    "\n",
    "encoding_strategies = {\n",
    "    'target_encoding': {}, # smoothed target mean by category group\n",
    "    'frequency_encoding': {}\n",
    "}\n",
    "\n",
    "# Test both encoding strategies\n",
    "for strategy_name in encoding_strategies.keys():\n",
    "    X_train_temp = X_train.copy()\n",
    "    X_test_temp = X_test.copy()\n",
    "\n",
    "    for col in high_card:\n",
    "        if col in X_train_orig.columns:\n",
    "            if strategy_name == 'target_encoding':\n",
    "                # Target encoding with K-Fold\n",
    "                kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "                train_encoded = np.zeros(len(X_train_orig))\n",
    "                global_mean = y_train.mean()\n",
    "\n",
    "                for train_idx, val_idx in kf.split(X_train_orig):\n",
    "                    X_fold = X_train_orig.iloc[train_idx]\n",
    "                    y_fold = y_train.iloc[train_idx]\n",
    "\n",
    "                    temp_df = pd.DataFrame({col: X_fold[col], 'target': y_fold.values})\n",
    "                    means = temp_df.groupby(col)['target'].agg(['mean', 'count'])\n",
    "                    smoothed = (means['mean'] * means['count'] + global_mean * smoothing) / (means['count'] + smoothing)\n",
    "\n",
    "                    train_encoded[val_idx] = X_train_orig.iloc[val_idx][col].map(smoothed).fillna(global_mean)\n",
    "\n",
    "                temp_df = pd.DataFrame({col: X_train_orig[col], 'target': y_train.values})\n",
    "                means = temp_df.groupby(col)['target'].agg(['mean', 'count'])\n",
    "                smoothed = (means['mean'] * means['count'] + global_mean * smoothing) / (means['count'] + smoothing)\n",
    "                test_encoded = X_test_orig[col].map(smoothed).fillna(global_mean)\n",
    "\n",
    "                X_train_temp[f'{col}_enc'] = train_encoded\n",
    "                X_test_temp[f'{col}_enc'] = test_encoded\n",
    "\n",
    "            else:  # frequency_encoding\n",
    "                # Frequency encoding\n",
    "                freq_map = X_train_orig[col].value_counts(normalize=True).to_dict()\n",
    "                X_train_temp[f'{col}_enc'] = X_train_orig[col].map(freq_map).fillna(0)\n",
    "                X_test_temp[f'{col}_enc'] = X_test_orig[col].map(freq_map).fillna(0)\n",
    "\n",
    "    # Quick model to evaluate\n",
    "    scaler_temp = RobustScaler()\n",
    "    X_train_scaled = scaler_temp.fit_transform(X_train_temp)\n",
    "    X_test_scaled = scaler_temp.transform(X_test_temp)\n",
    "\n",
    "    model_temp = Ridge(alpha=1.0, random_state=42)\n",
    "    model_temp.fit(X_train_scaled, y_train)\n",
    "\n",
    "    y_pred = model_temp.predict(X_test_scaled)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "    encoding_strategies[strategy_name]['rmse'] = rmse\n",
    "    encoding_strategies[strategy_name]['X_train'] = X_train_temp\n",
    "    encoding_strategies[strategy_name]['X_test'] = X_test_temp\n",
    "\n",
    "    print(f\"  {strategy_name:20s}: RMSE = {rmse:.4f}\")\n",
    "\n",
    "# Select best encoding strategy\n",
    "best_encoding = min(encoding_strategies.items(), key=lambda x: x[1]['rmse'])\n",
    "print(f\"\\n  Selected best: {best_encoding[0]} (RMSE: {best_encoding[1]['rmse']:.4f})\")\n",
    "\n",
    "# Apply best encoding\n",
    "X_train = best_encoding[1]['X_train']\n",
    "X_test = best_encoding[1]['X_test']\n",
    "\n",
    "encoded_high_card = [c for c in high_card if c in X_train_orig.columns]\n",
    "print(f\"  Applied {best_encoding[0]} for {len(encoded_high_card)} high-cardinality features: {encoded_high_card}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Removal via Cook's Distance\n",
    "\n",
    "Iteratively remove influential outliers using Cook's Distance with a conservative threshold to preserve data while removing extreme leverage points.\n",
    "\n",
    "Purpose: Remove influential outliers that distort the regression model. High Cook's D = removing this point significantly changes predictions for all other points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training samples: 21600\n",
      "Cook's Distance threshold: 0.000741\n",
      "  Iteration 1: Removed 181 outliers (Cook's D > 0.000741)\n",
      "  Iteration 2: Removed 89 outliers (Cook's D > 0.000741)\n",
      "  Iteration 3: Removed 36 outliers (Cook's D > 0.000741)\n",
      "\n",
      "Total outliers removed: 306\n",
      "Final training samples: 21294 (98.58% retained)\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Conservative Cook's Distance threshold (4/n is standard, we use 4*4/n for conservativeness)\n",
    "n = len(X_train)\n",
    "p = X_train.shape[1]\n",
    "cook_threshold = 4 * (4 / n)  # Very conservative: 16/n instead of 4/n\n",
    "\n",
    "print(f\"Initial training samples: {len(X_train)}\")\n",
    "print(f\"Cook's Distance threshold: {cook_threshold:.6f}\")\n",
    "\n",
    "max_iterations = 3  # Limit iterations to prevent over-removal\n",
    "iteration = 0\n",
    "outliers_removed_total = 0\n",
    "\n",
    "while iteration < max_iterations:\n",
    "    iteration += 1\n",
    "    \n",
    "    # Fit OLS model to calculate Cook's Distance\n",
    "    X_train_with_const = sm.add_constant(X_train)\n",
    "    model_ols = sm.OLS(y_train, X_train_with_const).fit()\n",
    "    \n",
    "    # Calculate Cook's Distance\n",
    "    influence = OLSInfluence(model_ols)\n",
    "    cooks_d = influence.cooks_distance[0]\n",
    "    \n",
    "    # Identify outliers\n",
    "    outlier_mask = cooks_d > cook_threshold\n",
    "    n_outliers = outlier_mask.sum()\n",
    "    \n",
    "    if n_outliers == 0:\n",
    "        print(f\"  Iteration {iteration}: No outliers found, stopping\")\n",
    "        break\n",
    "    \n",
    "    # Remove outliers from training data\n",
    "    X_train = X_train[~outlier_mask]\n",
    "    y_train = y_train[~outlier_mask]\n",
    "    outliers_removed_total += n_outliers\n",
    "    \n",
    "    print(f\"  Iteration {iteration}: Removed {n_outliers} outliers (Cook's D > {cook_threshold:.6f})\")\n",
    "    \n",
    "    # Recalculate n for next iteration\n",
    "    n = len(X_train)\n",
    "\n",
    "print(f\"\\nTotal outliers removed: {outliers_removed_total}\")\n",
    "print(f\"Final training samples: {len(X_train)} ({100 * len(X_train) / (len(X_train) + outliers_removed_total):.2f}% retained)\")\n",
    "\n",
    "# Note: X_test remains unchanged (we don't remove test outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Robust scaling applied\n"
     ]
    }
   ],
   "source": [
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_final = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_final = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(f\"  Robust scaling applied\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ridge(alpha=1.0, random_state=42)\n",
    "model.fit(X_train_final, y_train)\n",
    "\n",
    "# Predictions\n",
    "train_pred = model.predict(X_train_final)\n",
    "test_pred = model.predict(X_test_final)\n",
    "\n",
    "# Metrics\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "test_r2 = r2_score(y_test, test_pred)\n",
    "\n",
    "# Convert to RUB\n",
    "test_pred_rub = np.expm1(test_pred)\n",
    "y_test_rub = np.expm1(y_test)\n",
    "test_rmse_rub = np.sqrt(mean_squared_error(y_test_rub, test_pred_rub))\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(model, X_train_final, y_train, cv=5,\n",
    "                            scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "cv_rmse = np.sqrt(-cv_scores).mean()\n",
    "cv_rmse_std = np.sqrt(-cv_scores).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RESULTS\n",
      "================================================================================\n",
      "Features:       68\n",
      "Samples:        21294\n",
      "\n",
      "Test RMSE:      0.4695 (log) | 3,082,742 RUB\n",
      "5-Fold CV RMSE MEAN: 0.4566 (log)\n",
      "5-Fold CV RMSE STD: 0.0085 (log)\n",
      "\n",
      "Baseline RMSE:  5,626,709 RUB\n",
      "Improvement:    +45.21%\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Features:       {X_train_final.shape[1]}\")\n",
    "print(f\"Samples:        {len(y_train)}\")\n",
    "print(f\"\\nTest RMSE:      {test_rmse:.4f} (log) | {test_rmse_rub:,.0f} RUB\")\n",
    "print(f\"5-Fold CV RMSE MEAN: {cv_rmse:.4f} (log)\")\n",
    "print(f\"5-Fold CV RMSE STD: {cv_rmse_std:.4f} (log)\")\n",
    "\n",
    "# Compare with baseline\n",
    "try:\n",
    "    with open(\"baseline_rmse.txt\", \"r\") as f:\n",
    "        baseline_rmse_rub = float(f.read().strip())\n",
    "    improvement = ((baseline_rmse_rub - test_rmse_rub) / baseline_rmse_rub) * 100\n",
    "    print(f\"\\nBaseline RMSE:  {baseline_rmse_rub:,.0f} RUB\")\n",
    "    print(f\"Improvement:    {improvement:+.2f}%\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Calculate residuals\n",
    "# residuals = y_test - test_pred\n",
    "\n",
    "# # 1. Check homoscedasticity\n",
    "# plt.figure(figsize=(12, 5))\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.scatter(test_pred, residuals, alpha=0.6)\n",
    "# plt.xlabel(\"Fitted Values (Predictions)\")\n",
    "# plt.ylabel(\"Residuals\")\n",
    "# plt.title(\"Residuals vs Fitted\")\n",
    "# plt.axhline(y=0, color='red', linestyle='--')\n",
    "\n",
    "# # 2. Check normality of residuals\n",
    "# plt.subplot(1, 2, 2)\n",
    "# stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "# plt.title(\"Q-Q Plot - Residual Normality\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
